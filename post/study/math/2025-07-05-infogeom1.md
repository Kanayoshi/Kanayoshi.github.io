---
layout: default
title: "infogeom1"
date: 2025-07-05
use_mathjax: true
# categories:
---

# 情報幾何メモ(1)

勉強会で [Amari, "Information Geometry and Its Application"](https://link.springer.com/book/10.1007/978-4-431-55978-8) を読み始めている。先日 1.2 節を発表した際、参加者の方々より有益なコメントをいただいたため、いくつかメモ。 

## Divergence の定義について

Amari では divergence の定義として 3 つ与えているが、(3) の定義

> $P, Q \in M$ が十分近い、 すなわち $\xi_{Q} = \xi_{P} + d\xi$ のとき、$D$ のテイラー展開
> $$
>   D[\xi_{P} : \xi_{P} + d\xi] = \frac{1}{2}\sum g_{ij}(\xi_{P}) d\xi_{i}d\xi_{j} + O(|d\xi|^3)
> $$
> に現れる係数行列 $G = (g_{ij})$ は正定値である.

が (1), (2) から従いそうに見えるが, この条件は本質的である. 
実際, 次の節で紹介されている Euclidian Divergence について, 定義を
$$
    D'[P : Q] = \frac{1}{2} \sum (\xi_{Pi} - \xi_{Qi})^{\textbf{4}}
$$
に変更すると, divergence の定義の内、(1), (2) は満たすが (3) は満たさない例ができる（零行列は正定値ではない）. 

## KL divergence　について

KL divergence が divergence の定義 (1), (2) を満たすことを確かめるためには、 Jensen の不等式を示せば良い。ここの導出で手間取った（申し訳ない）ので、改めてここで残しておく。

ここで使用する Jensen の不等式は以下の通り。

$f(x)$ を上に凸な関数、$g(x)$ を任意の関数、$p(x)$ を （連続な）確率分布とする。このとき、
$$
    f\left(\int p(x)g(x) dx\right) \geq \int p(x)f(g(x)) dx
$$
が成り立つ。

$f(x) = \log x$、$g(x) = q(x)/p(x)$ とすると、

$$
\begin{align*}
    \log\left(\int p(x) \times \frac{q(x)}{p(x)} dx \right) &\geq \int p(x) \log\left(\frac{q(x)}{p(x)}\right)\\
    0 = \log 1 = \log \left(\int q(x)dx \right) &\geq  - \int p(x) \log\left(\frac{p(x)}{q(x)}\right)\\
    0 &\leq D_{KL}[P : Q]
\end{align*}
$$

となり、divergence の定義 (1) が成り立つ.
また、Jensen の不等式の等号成立条件は $f(x)$ が線形、または $g(x)$ が定数関数であることから、今の場合等号が成立する条件は
$$
    g(x) = \frac{q(x)}{p(x)} = c
$$
（$c$ は定数）であるが、両辺積分すると
$$
     1 = \int q(x) dx = c\int p(x) dx = c
$$
となり、$p(x) = q(x)$ が得られる。これより、divergence の定義 (2) が示される。

(3) については適当な離散分布や正規分布の場合に確認できるというコメントをいただいた。時間があるときに手元で確かめておきたい。